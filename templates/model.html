<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model & Evaluation</title>

    <!-- Static CSS -->
    <link rel="stylesheet" href="/static/base.css">
</head>

<body>

    <div class="page-container glass">

        <a href="/">← Back to Home</a>

        <h1>Model & Evaluation</h1>

        <p>
            This section describes the modeling approach used to estimate tail
            risk in insurance claim severity and the metrics employed to evaluate
            performance. The focus here is on the learning objective, model
            suitability for the dataset, and the reliability of predictions in
            the upper tail rather than on average error minimization.
        </p>

        <h2>Model Choice: CatBoost Quantile Regression</h2>

        <img
            src="/assets/catboost.png"
            alt="CatBoost Logo"
            style="
                width: 220px;
                margin: 20px 0 28px 0;
                display: block;
            "
        >

        <p>
            The Allstate Claims Severity dataset contains a high-dimensional mix
            of continuous and categorical variables with anonymized semantics.
            This makes linear models and naive one-hot encoding approaches
            unsuitable due to feature explosion and limited capacity to model
            non-linear interactions.
        </p>

        <p>
            CatBoost was selected due to its native handling of categorical
            features via ordered target statistics, which reduces target leakage
            while avoiding sparse high-dimensional representations. As a
            tree-based ensemble method, it is well-suited for modeling complex
            interactions and skewed loss distributions commonly observed in
            insurance data.
        </p>

        <h2>Quantile Objective (τ = 0.90)</h2>

        <p>
            Instead of predicting the conditional mean of claim severity, the
            model is trained to estimate the conditional 90th percentile
            (τ = 0.90). For a given claim profile, the prediction represents a
            threshold that 90% of comparable claims are expected not to exceed.
        </p>

        <p>
            This objective directly aligns with tail-risk estimation, where the
            cost of underestimating large claims is significantly higher than
            overestimating smaller ones.
        </p>

        <h2>Evaluation Metrics</h2>

        <h2>Pinball Loss</h2>

        <p>
            Pinball loss is the standard loss function used in quantile
            regression. Unlike symmetric error metrics, it applies asymmetric
            penalties depending on whether predictions fall above or below the
            target, with the degree of asymmetry controlled by the quantile
            level τ.
        </p>

        <p>
            For τ = 0.90, underprediction is penalized more heavily than
            overprediction, reflecting the higher cost associated with
            underestimating extreme insurance losses. This makes pinball loss
            particularly suitable for tail-focused risk modeling.
        </p>

        <p>
            <strong>Interpretation:</strong> Lower values indicate better
            performance. There is no fixed upper bound, and values are scale-
            dependent on the target variable. Pinball loss should be compared
            across models trained on the same data and quantile.
        </p>

        <img
            src="/assets/quantile_loss.jpeg"
            alt="Pinball Loss Visualization"
            style="
                width: 100%;
                max-width: 400px;
                margin: 16px 0 24px 0;
                display: block;
            "
        >

        <p>
            The model achieves a pinball loss of <strong>337.9054</strong>,
            indicating stable learning behavior under a highly skewed loss
            distribution and controlled error in the upper tail.
        </p>

        <h2>D² (Quantile R²)</h2>

        <p>
            D² is a quantile analogue of the traditional R² metric. It measures
            the relative improvement of the trained model over a naive baseline
            that predicts an unconditional quantile, rather than the mean.
        </p>

        <p>
            This metric captures how much explanatory power the model provides
            when estimating conditional quantiles, making it more appropriate
            than classical R² for quantile regression tasks.
        </p>

        <p>
            <strong>Interpretation:</strong> D² typically ranges from negative
            values to 1. A value close to 0 indicates little improvement over
            the baseline, while higher positive values indicate stronger
            predictive power. Higher is better.
        </p>

        <img
            src="/assets/d2_pinball_score.jpeg"
            alt="D² Score Visualization"
            style="
                width: 100%;
                max-width: 400px;
                margin: 16px 0 24px 0;
                display: block;
            "
        >

        <p>
            The obtained D² value of <strong>0.4859</strong> suggests that the
            model explains a substantial portion of the variation in the
            conditional 90th percentile of claim severity.
        </p>

        <h2>Coverage</h2>

        <p>
            Coverage evaluates the calibration of a quantile regression model by
            measuring the proportion of observed outcomes that fall below the
            predicted quantile. For a well-calibrated model, empirical coverage
            should closely match the nominal quantile level.
        </p>

        <p>
            In practice, coverage is used to assess whether the model
            systematically underestimates or overestimates risk in the tail.
            This metric is particularly important for decision-making contexts
            such as reserving and stress testing.
        </p>

        <p>
            <strong>Interpretation:</strong> Coverage values range between 0 and
            1. For a τ = 0.90 model, values close to 0.90 indicate good
            calibration. Values significantly below the target suggest
            underestimation of risk, while values well above suggest overly
            conservative predictions.
        </p>

        <img
            src="/assets/coverage.png"
            alt="Coverage Calibration Visualization"
            style="
                width: 100%;
                max-width: 720px;
                margin: 16px 0 24px 0;
                display: block;
            "
        >

        <p>
            The model achieves an empirical coverage of <strong>0.8940</strong>,
            which is very close to the nominal target, indicating reliable
            tail-risk calibration.
        </p>

        <h2>Key Takeaways & Limitations</h2>

        <p>
            Overall, the CatBoost-based quantile regression model demonstrates
            strong performance in estimating upper-tail claim severity, with
            metrics indicating both predictive power and good calibration.
        </p>

        <p>
            The current evaluation is based on a single train–validation split.
            Future work will focus on cross-validation, additional tail
            diagnostics, and robustness checks under distributional shifts.
        </p>

    </div>

    <div class="footer">
        © 2026 · Tail Risk Modeling Project
    </div>

</body>
</html>
